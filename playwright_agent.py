import asyncio
import base64
import json
import time
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from datetime import datetime
import os
import logging

from playwright.async_api import async_playwright, Page, Browser, BrowserContext
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage
from PIL import Image
import io

from logger_config import setup_logging

logger = setup_logging(__name__)

@dataclass
class ProductData:
    """Standardized product data format (matches existing system)"""
    title: str = ""
    brand: str = ""
    price: Optional[float] = None
    original_price: Optional[float] = None
    description: str = ""
    stock_status: str = ""
    sale_status: str = ""
    clothing_type: str = ""
    product_code: str = ""
    image_urls: List[str] = None
    retailer: str = ""
    colors: List[str] = None
    sizes: List[str] = None
    material: str = ""
    care_instructions: str = ""
    
    def __post_init__(self):
        if self.image_urls is None:
            self.image_urls = []
        if self.colors is None:
            self.colors = []
        if self.sizes is None:
            self.sizes = []

@dataclass
class ExtractionResult:
    """Matches existing system's ExtractionResult format"""
    success: bool
    data: Dict[str, Any]
    method_used: str
    processing_time: float
    warnings: List[str]
    errors: List[str]

class PlaywrightMultiScreenshotAgent:
    """
    Complete replacement for Browser Use using Playwright + Multi-Screenshot + Gemini 2.0 Flash
    Handles verification challenges, anti-scraping, and provides better performance
    """
    
    def __init__(self, config: Dict = None):
        """Initialize with existing system configuration"""
        self.config = config or self._load_config()
        self.browser: Optional[Browser] = None
        self.context: Optional[BrowserContext] = None
        
        # Initialize Gemini 2.0 Flash (use existing config)
        self._setup_gemini()
        
        # Retailer-specific screenshot strategies
        self.screenshot_strategies = self._load_screenshot_strategies()
        
        # Anti-scraping settings
        self.stealth_enabled = True
        self.max_retries = 3
        
    def _load_config(self) -> Dict:
        """Load configuration from config.json"""
        try:
            config_path = os.path.join(os.path.dirname(__file__), 'config.json')
            with open(config_path, 'r') as f:
                config = json.load(f)
            logger.info("‚úÖ Configuration loaded from config.json")
            return config
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to load config.json: {e}")
            return {}
    
    def _setup_gemini(self):
        """Setup Gemini 2.0 Flash using existing configuration"""
        try:
            # Use existing API key from config
            api_key = os.getenv("GOOGLE_API_KEY") or self.config.get("llm_providers", {}).get("google", {}).get("api_key")
            if not api_key:
                raise ValueError("No Google API key found")
            
            # Use langchain_google_genai (same as existing system)
            self.model = ChatGoogleGenerativeAI(
                model="gemini-2.0-flash-exp",
                api_key=api_key,
                temperature=0.1,
                max_output_tokens=2048,
                top_p=0.8,
                top_k=40
            )
            logger.info("‚úÖ Gemini 2.0 Flash initialized for Playwright agent")
            
        except Exception as e:
            logger.error(f"Failed to initialize Gemini: {e}")
            raise
    
    def _load_screenshot_strategies(self) -> Dict:
        """Load retailer-specific screenshot strategies"""
        return {
            'aritzia.com': {
                'screenshots': ['full_page', 'main_image', 'product_details', 'size_color_options'],
                'anti_scraping': 'medium',
                'wait_conditions': ['networkidle', '.product-name, .product-title'],
                'scroll_positions': [0, 0.3, 0.6, 1.0],
                'cloudflare_likely': True
            },
            'anthropologie.com': {
                'screenshots': ['full_page', 'hero_section', 'product_info', 'image_gallery'],
                'anti_scraping': 'high',
                'wait_conditions': ['load', '.product-name, .product-title'],
                'scroll_positions': [0, 0.4, 0.8],
                'verification_challenge': 'press_and_hold'
            },
            'nordstrom.com': {
                'screenshots': ['full_page', 'main_content', 'product_details', 'images_section'],
                'anti_scraping': 'very_high',  # Known to block Browser Use
                'wait_conditions': ['networkidle', '.product-title, .product-name'],
                'scroll_positions': [0, 0.5, 1.0],
                'ip_blocking': True
            },
            'urbanoutfitters.com': {
                'screenshots': ['full_page', 'product_hero', 'details_section', 'related_images'],
                'anti_scraping': 'high',
                'wait_conditions': ['load', '.product-title, .product-name'],
                'scroll_positions': [0, 0.3, 0.7],
                'verification_challenge': 'press_and_hold'
            },
            'abercrombie.com': {
                'screenshots': ['full_page', 'main_product', 'product_info', 'image_carousel'],
                'anti_scraping': 'medium',
                'wait_conditions': ['networkidle', '.product-name, .product-title'],
                'scroll_positions': [0, 0.4, 0.8],
                'category_page_handling': True
            }
        }
    
    async def extract_product(self, url: str, retailer: str) -> ExtractionResult:
        """
        Main extraction method - replaces Browser Use completely
        """
        start_time = time.time()
        logger.info(f"üé≠ Starting Playwright extraction for {retailer}: {url}")
        
        try:
            # Setup stealth browser
            await self._setup_stealth_browser()
            
            # Extract with retry logic
            result = await self._extract_with_retry(url, retailer)
            
            processing_time = time.time() - start_time
            logger.info(f"‚úÖ Playwright extraction completed in {processing_time:.1f}s")
            
            return ExtractionResult(
                success=True,
                data=result.__dict__,
                method_used="playwright_multi_screenshot",
                processing_time=processing_time,
                warnings=[],
                errors=[]
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"‚ùå Playwright extraction failed: {e}")
            
            return ExtractionResult(
                success=False,
                data={},
                method_used="playwright_multi_screenshot",
                processing_time=processing_time,
                warnings=[],
                errors=[str(e)]
            )
        
        finally:
            await self._cleanup()
    
    async def _setup_stealth_browser(self):
        """Setup Playwright browser with advanced anti-detection"""
        try:
            # Import stealth (install if needed)
            try:
                from playwright_stealth import stealth_async
                self.stealth_available = True
            except ImportError:
                logger.warning("playwright-stealth not available, using basic stealth")
                self.stealth_available = False
            
            playwright = await async_playwright().start()
            
            # Launch browser with anti-detection args
            self.browser = await playwright.chromium.launch(
                headless=True,
                args=[
                    '--disable-blink-features=AutomationControlled',
                    '--disable-dev-shm-usage',
                    '--no-sandbox',
                    '--disable-gpu',
                    '--disable-extensions',
                    '--disable-plugins',
                    '--disable-images',  # Faster loading
                    '--disable-javascript',  # Avoid verification triggers
                    '--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
                ]
            )
            
            # Create context with realistic settings
            self.context = await self.browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                extra_http_headers={
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Language': 'en-US,en;q=0.5',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'DNT': '1',
                    'Connection': 'keep-alive',
                    'Upgrade-Insecure-Requests': '1'
                }
            )
            
            # Apply stealth if available
            if self.stealth_available:
                page = await self.context.new_page()
                await stealth_async(page)
                self.page = page
            else:
                self.page = await self.context.new_page()
            
            logger.info("ü•∑ Stealth browser setup complete")
            
        except Exception as e:
            logger.error(f"Failed to setup stealth browser: {e}")
            raise
    
    async def _extract_with_retry(self, url: str, retailer: str) -> ProductData:
        """Extract with retry logic for verification challenges"""
        last_error = None
        
        for attempt in range(self.max_retries):
            try:
                logger.info(f"üîÑ Attempt {attempt + 1}/{self.max_retries} for {retailer}")
                
                # Navigate with smart waiting
                result = await self._navigate_and_capture(url, retailer)
                
                if result:
                    logger.info(f"‚úÖ Success on attempt {attempt + 1}")
                    return result
                    
            except Exception as e:
                last_error = e
                logger.warning(f"‚ö†Ô∏è Attempt {attempt + 1} failed: {e}")
                
                if attempt < self.max_retries - 1:
                    # Progressive delay
                    delay = 2 ** attempt
                    logger.info(f"üïí Waiting {delay}s before retry...")
                    await asyncio.sleep(delay)
                    
                    # Reset browser context for fresh attempt
                    await self._reset_browser_context()
        
        raise Exception(f"All {self.max_retries} attempts failed. Last error: {last_error}")
    
    async def _navigate_and_capture(self, url: str, retailer: str) -> ProductData:
        """Navigate to page and capture strategic screenshots"""
        
        # Get retailer strategy
        domain = self._extract_domain(url)
        strategy = self.screenshot_strategies.get(domain, self.screenshot_strategies.get('default', {
            'screenshots': ['full_page', 'main_content'],
            'anti_scraping': 'medium',
            'wait_conditions': ['load'],
            'scroll_positions': [0, 0.5, 1.0]
        }))
        
        logger.info(f"üìã Using strategy for {domain}: {strategy.get('anti_scraping')} anti-scraping")
        
        # Navigate with timeout and error handling
        try:
            logger.info(f"üåê Navigating to: {url}")
            response = await self.page.goto(url, wait_until='load', timeout=30000)
            
            if response.status >= 400:
                logger.warning(f"‚ö†Ô∏è HTTP {response.status} response")
            
        except Exception as e:
            logger.error(f"‚ùå Navigation failed: {e}")
            raise
        
        # Wait for content to load
        await self._wait_for_content(strategy)
        
        # Handle verification challenges proactively
        await self._handle_verification_challenges(strategy)
        
        # Take strategic screenshots
        screenshots = await self._take_strategic_screenshots(strategy, retailer)
        
        if not screenshots:
            raise Exception("No screenshots captured")
        
        # Analyze with Gemini
        product_data = await self._analyze_with_gemini(screenshots, url, retailer)
        
        return product_data
    
    async def _wait_for_content(self, strategy: Dict):
        """Smart waiting based on retailer strategy"""
        wait_conditions = strategy.get('wait_conditions', ['load'])
        
        for condition in wait_conditions:
            try:
                if condition == 'networkidle':
                    await self.page.wait_for_load_state('networkidle', timeout=15000)
                elif condition == 'load':
                    await self.page.wait_for_load_state('load', timeout=10000)
                elif condition.startswith('.') or condition.startswith('#'):
                    # CSS selector
                    await self.page.wait_for_selector(condition, timeout=10000)
                
                logger.debug(f"‚úÖ Wait condition met: {condition}")
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Wait condition failed: {condition} - {e}")
                continue
    
    async def _handle_verification_challenges(self, strategy: Dict):
        """Proactively handle verification challenges that break Browser Use"""
        
        # Check for common verification elements
        verification_selectors = [
            'button:has-text("Press & Hold")',
            'button:has-text("Verify")',
            '.captcha-container',
            '.cloudflare-browser-verification',
            '#challenge-form',
            '[data-testid="press-hold-button"]'
        ]
        
        for selector in verification_selectors:
            try:
                element = await self.page.query_selector(selector)
                if element:
                    logger.warning(f"üõ°Ô∏è Verification challenge detected: {selector}")
                    
                    # Try gentle interaction (unlike Browser Use's aggressive approach)
                    if 'press' in selector.lower() and 'hold' in selector.lower():
                        await self._handle_press_and_hold(element)
                    elif 'verify' in selector.lower():
                        await element.click()
                        await asyncio.sleep(2)
                    
                    # Wait to see if challenge resolves
                    await asyncio.sleep(3)
                    break
                    
            except Exception as e:
                logger.debug(f"Verification check failed for {selector}: {e}")
                continue
    
    async def _handle_press_and_hold(self, element):
        """Handle press-and-hold verification (Browser Use's biggest failure)"""
        try:
            logger.info("üîò Attempting press-and-hold verification...")
            
            # Method 1: Mouse down and hold
            box = await element.bounding_box()
            if box:
                x = box['x'] + box['width'] / 2
                y = box['y'] + box['height'] / 2
                
                await self.page.mouse.move(x, y)
                await self.page.mouse.down()
                await asyncio.sleep(5)  # Hold for 5 seconds
                await self.page.mouse.up()
                
                logger.info("‚úÖ Press-and-hold completed")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Press-and-hold failed: {e}")
    
    async def _take_strategic_screenshots(self, strategy: Dict, retailer: str) -> Dict[str, bytes]:
        """Take multiple strategic screenshots for comprehensive analysis"""
        screenshots = {}
        screenshot_types = strategy.get('screenshots', ['full_page'])
        scroll_positions = strategy.get('scroll_positions', [0, 0.5, 1.0])
        
        logger.info(f"üì∏ Taking {len(screenshot_types)} types of screenshots")
        
        try:
            # Full page screenshot first
            if 'full_page' in screenshot_types:
                screenshot = await self.page.screenshot(full_page=True, type='png')
                screenshots['full_page'] = screenshot
                logger.debug("‚úÖ Full page screenshot captured")
            
            # Viewport screenshots at different scroll positions
            for i, position in enumerate(scroll_positions):
                await self._scroll_to_position(position)
                await asyncio.sleep(1)  # Let content load
                
                screenshot = await self.page.screenshot(type='png')
                screenshots[f'viewport_scroll_{position}'] = screenshot
                logger.debug(f"‚úÖ Viewport screenshot captured at scroll {position}")
            
            # Specific element screenshots if selectors are available
            element_selectors = {
                'main_image': '.product-image, .hero-image, .main-image, [data-testid="product-image"]',
                'product_details': '.product-details, .product-info, .product-description',
                'price_section': '.price, .product-price, .pricing',
                'image_gallery': '.image-gallery, .product-images, .carousel'
            }
            
            for screenshot_type in screenshot_types:
                if screenshot_type in element_selectors:
                    selector = element_selectors[screenshot_type]
                    try:
                        element = await self.page.query_selector(selector)
                        if element:
                            screenshot = await element.screenshot(type='png')
                            screenshots[screenshot_type] = screenshot
                            logger.debug(f"‚úÖ Element screenshot captured: {screenshot_type}")
                    except Exception as e:
                        logger.debug(f"Element screenshot failed for {screenshot_type}: {e}")
            
            logger.info(f"üì∏ Captured {len(screenshots)} screenshots total")
            return screenshots
            
        except Exception as e:
            logger.error(f"‚ùå Screenshot capture failed: {e}")
            raise
    
    async def _scroll_to_position(self, position: float):
        """Scroll to specific position on page (0.0 = top, 1.0 = bottom)"""
        try:
            await self.page.evaluate(f"""
                window.scrollTo({{
                    top: document.body.scrollHeight * {position},
                    behavior: 'smooth'
                }});
            """)
        except Exception as e:
            logger.debug(f"Scroll to {position} failed: {e}")
    
    async def _analyze_with_gemini(self, screenshots: Dict[str, bytes], url: str, retailer: str) -> ProductData:
        """Analyze all screenshots with single Gemini 2.0 Flash call"""
        logger.info(f"üß† Analyzing {len(screenshots)} screenshots with Gemini 2.0 Flash")
        
        try:
            # Prepare images for Gemini
            images = []
            screenshot_descriptions = []
            
            for name, screenshot_bytes in screenshots.items():
                # Convert to PIL Image and resize if too large
                image = Image.open(io.BytesIO(screenshot_bytes))
                
                # Resize if too large (Gemini limits)
                if image.width > 2048 or image.height > 2048:
                    image.thumbnail((2048, 2048), Image.Resampling.LANCZOS)
                
                images.append(image)
                screenshot_descriptions.append(name)
            
            # Construct comprehensive prompt
            prompt = self._build_analysis_prompt(url, retailer, screenshot_descriptions)
            
            # Single Gemini API call for all analysis
            response = await self._call_gemini_with_images(prompt, images)
            
            # Parse response into ProductData
            product_data = self._parse_gemini_response(response, retailer)
            
            logger.info(f"‚úÖ Gemini analysis complete: {product_data.title}")
            return product_data
            
        except Exception as e:
            logger.error(f"‚ùå Gemini analysis failed: {e}")
            raise
    
    def _build_analysis_prompt(self, url: str, retailer: str, screenshot_types: List[str]) -> str:
        """Build comprehensive analysis prompt for Gemini"""
        return f"""
You are analyzing a clothing product page from {retailer}. URL: {url}

I'm providing {len(screenshot_types)} screenshots: {', '.join(screenshot_types)}

Extract ALL available product information and return as JSON with this exact structure:

{{
    "title": "Full product name",
    "brand": "Brand name",
    "price": 99.99,
    "original_price": 149.99,
    "description": "Full product description",
    "stock_status": "in_stock/low_stock/out_of_stock",
    "sale_status": "on_sale/regular_price",
    "clothing_type": "dress/top/bottom/etc",
    "product_code": "SKU or product ID",
    "image_urls": ["url1", "url2", "url3"],
    "colors": ["color1", "color2"],
    "sizes": ["XS", "S", "M", "L", "XL"],
    "material": "fabric composition",
    "care_instructions": "washing instructions",
    "retailer": "{retailer}"
}}

CRITICAL REQUIREMENTS:
1. Extract ALL image URLs you can find (product images, zoom images, color variants)
2. Rank images by importance: main product shot first, then detail views
3. Look for hidden/lazy-loaded image URLs in data attributes
4. Find product codes/SKUs in URLs, data attributes, or text
5. Extract exact pricing (sale vs original)
6. Identify all available colors and sizes
7. Return valid JSON only, no extra text

Focus on extracting comprehensive product data and ALL available image URLs.
"""
    
    async def _call_gemini_with_images(self, prompt: str, images: List[Image.Image]) -> str:
        """Make single Gemini API call with all images using langchain"""
        try:
            # Convert images to base64 for langchain
            image_content = []
            
            for image in images:
                # Convert PIL Image to bytes
                img_byte_arr = io.BytesIO()
                image.save(img_byte_arr, format='PNG')
                img_byte_arr = img_byte_arr.getvalue()
                
                # Convert to base64
                img_base64 = base64.b64encode(img_byte_arr).decode('utf-8')
                
                image_content.append({
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/png;base64,{img_base64}"
                    }
                })
            
            # Prepare message with text and images
            content = [{"type": "text", "text": prompt}] + image_content
            
            message = HumanMessage(content=content)
            
            # Call Gemini via langchain
            response = await asyncio.to_thread(self.model.invoke, [message])
            
            return response.content
            
        except Exception as e:
            logger.error(f"Gemini API call failed: {e}")
            raise
    
    def _parse_gemini_response(self, response: str, retailer: str) -> ProductData:
        """Parse Gemini response into ProductData object"""
        try:
            # Extract JSON from response
            json_start = response.find('{')
            json_end = response.rfind('}') + 1
            
            if json_start == -1 or json_end == 0:
                raise ValueError("No JSON found in response")
            
            json_str = response[json_start:json_end]
            data = json.loads(json_str)
            
            # Process and standardize the data using the same logic as the old system
            processed_data = self._process_extracted_data(data, retailer)
            
            # Create ProductData object with processed data
            product_data = ProductData(
                title=processed_data.get('title', ''),
                brand=processed_data.get('brand', ''),
                price=processed_data.get('price'),
                original_price=processed_data.get('original_price'),
                description=processed_data.get('description', ''),
                stock_status=processed_data.get('stock_status', ''),
                sale_status=processed_data.get('sale_status', ''),
                clothing_type=processed_data.get('clothing_type', ''),
                product_code=processed_data.get('product_code', ''),
                image_urls=processed_data.get('image_urls', []),
                retailer=retailer,
                colors=processed_data.get('colors', []),
                sizes=processed_data.get('sizes', []),
                material=processed_data.get('material', ''),
                care_instructions=processed_data.get('care_instructions', '')
            )
            
            return product_data
            
        except Exception as e:
            logger.error(f"Failed to parse Gemini response: {e}")
            logger.debug(f"Response was: {response[:500]}...")
            
            # Return minimal ProductData on parse failure
            return ProductData(
                title="Parse Error",
                retailer=retailer
            )
    
    def _process_extracted_data(self, data: dict, retailer: str) -> dict:
        """Apply retailer-specific data cleaning and formatting (from old system)"""
        
        # Ensure data is a dictionary
        if not isinstance(data, dict):
            logger.warning(f"Expected dict but got {type(data)}: {data}")
            data = {
                "retailer": retailer,
                "title": "Extracted Product",
                "raw_output": str(data)
            }
        
        processed = data.copy()
        
        # Clean price formats
        if 'price' in processed:
            processed['price'] = self._clean_price_format(processed['price'], retailer)
        
        if 'original_price' in processed:
            processed['original_price'] = self._clean_price_format(processed['original_price'], retailer)
        
        # Special handling for Uniqlo - they don't show original prices during sales
        if retailer == 'uniqlo':
            sale_status = processed.get('sale_status', '')
            # Handle both boolean and string sale_status
            is_on_sale = False
            if isinstance(sale_status, bool):
                is_on_sale = sale_status
            elif isinstance(sale_status, str) and sale_status and 'sale' in sale_status.lower():
                is_on_sale = True
            
            if is_on_sale:
                # If item is on sale but no original price provided, set to None
                if not processed.get('original_price'):
                    processed['original_price'] = None
                    logger.debug(f"Uniqlo sale item without original price: {processed.get('title', 'Unknown')}")
        
        # Clean title
        if 'title' in processed:
            processed['title'] = self._clean_title_format(processed['title'], retailer)
        
        # Clean brand
        if 'brand' in processed:
            processed['brand'] = self._clean_brand_format(processed['brand'], retailer)
        
        # Standardize status fields
        if 'stock_status' in processed:
            processed['stock_status'] = self._standardize_stock_status(processed['stock_status'])
        
        if 'sale_status' in processed:
            processed['sale_status'] = self._standardize_sale_status(processed['sale_status'])
        
        if 'clothing_type' in processed:
            processed['clothing_type'] = self._standardize_clothing_type(processed['clothing_type'])
        
        # Validate retailer name
        processed['retailer'] = retailer  # Ensure consistency
        
        return processed
    
    def _clean_price_format(self, price_string, retailer: str) -> Optional[float]:
        """Clean and validate price format (from old system)"""
        if not price_string:
            return None
        
        # Handle numeric input
        if isinstance(price_string, (int, float)):
            return float(price_string) if price_string > 0 else None
        
        # Retailer-specific price patterns
        patterns = {
            "aritzia": r'CAD\s*\$?(\d+(?:\.\d{2})?)',
            "mango": r'US\$\s*(\d+(?:[.,]\d{2})?)|(\d+(?:[.,]\d{2})?)\s*‚Ç¨',
            "hm": r'¬£(\d+(?:\.\d{2})?)|\$(\d+(?:\.\d{2})?)',
            "revolve": r'\$(\d+(?:\.\d{2})?)',
            "nordstrom": r'\$(\d+(?:\.\d{2})?)',
            "anthropologie": r'\$(\d+(?:\.\d{2})?)',
            "default": r'[\$¬£‚Ç¨]?(\d+(?:[.,]\d{2})?)'
        }
        
        pattern = patterns.get(retailer, patterns["default"])
        
        # Clean the price string
        import re
        clean_price = str(price_string).replace(',', '').replace(' ', '')
        
        match = re.search(pattern, clean_price)
        
        if match:
            try:
                # Get the first non-None group
                price_value = next((group for group in match.groups() if group is not None), None)
                if price_value:
                    return float(price_value.replace(',', '.'))
            except (ValueError, AttributeError):
                pass
        
        # Fallback: try to extract any number
        fallback_match = re.search(r'(\d+(?:[.,]\d{2})?)', clean_price)
        if fallback_match:
            try:
                return float(fallback_match.group(1).replace(',', '.'))
            except ValueError:
                pass
        
        return None
    
    def _clean_title_format(self, title: str, retailer: str) -> str:
        """Clean retailer-specific title formats (from old system)"""
        if not title:
            return ""
        
        import re
        # Remove retailer suffixes
        cleaning_rules = {
            "aritzia": lambda t: re.sub(r'\s+\|\s+Aritzia.*$', '', t),
            "asos": lambda t: re.sub(r'\s+\|\s+ASOS.*$', '', t),
            "hm": lambda t: re.sub(r'\s+-\s+H&M.*$', '', t),
        }
        
        cleaner = cleaning_rules.get(retailer, lambda t: t)
        return cleaner(title).strip()
    
    def _clean_brand_format(self, brand: str, retailer: str) -> str:
        """Clean and validate brand names (from old system)"""
        if not brand:
            return retailer.title()
        
        import re
        # Remove retailer suffixes and validate
        brand = re.sub(r'\s+\|\s+.*$', '', brand).strip()
        
        # Retailer-specific brand validation
        valid_brands = {
            "aritzia": ["Aritzia", "TNA", "Wilfred", "Babaton"],
            "hm": ["H&M"],
            "uniqlo": ["Uniqlo"],
            "mango": ["Mango"]
        }
        
        if retailer in valid_brands:
            if brand.lower() not in [b.lower() for b in valid_brands[retailer]]:
                return valid_brands[retailer][0]  # Default to main brand
        
        return brand
    
    def _standardize_stock_status(self, status) -> str:
        """Standardize stock status - handles various input types (from old system)"""
        if status is None:
            return "in stock"
        
        # Handle boolean input (True = in stock, False = out of stock)
        if isinstance(status, bool):
            return 'in stock' if status else 'out of stock'
        
        # Handle string input
        if not isinstance(status, str):
            status = str(status)
        
        status_lower = status.lower()
        
        if any(indicator in status_lower for indicator in 
               ['sold out', 'unavailable', 'out of stock']):
            return 'out of stock'
        elif any(indicator in status_lower for indicator in 
                 ['few left', 'limited', 'low stock', 'almost gone']):
            return 'low in stock'
        else:
            return 'in stock'
    
    def _standardize_sale_status(self, status) -> str:
        """Standardize sale status - handles both string and boolean inputs (from old system)"""
        if status is None:
            return "not on sale"
        
        # Handle boolean input
        if isinstance(status, bool):
            return 'on sale' if status else 'not on sale'
        
        # Handle string input
        if not isinstance(status, str):
            status = str(status)
        
        sale_indicators = ['sale', 'clearance', 'reduced', 'discount', 'marked down', 'on sale']
        
        if any(indicator in status.lower() for indicator in sale_indicators):
            return 'on sale'
        return 'not on sale'
    
    def _standardize_clothing_type(self, clothing_type: str) -> str:
        """Standardize clothing type categories (from old system)"""
        if not clothing_type:
            return "clothing"
        
        type_mapping = {
            'dresses': 'dress',
            'tops & tees': 'top',
            'blouses': 'top',
            'shirts': 'top',
            'sweaters': 'top',
            'pants': 'bottom',
            'jeans': 'bottom',
            'skirts': 'bottom',
            'shorts': 'bottom',
            'jackets': 'outerwear',
            'coats': 'outerwear',
            'blazers': 'outerwear'
        }
        
        clothing_type_lower = clothing_type.lower()
        for key, value in type_mapping.items():
            if key in clothing_type_lower:
                return value
        
        return clothing_type.lower()
    
    async def _reset_browser_context(self):
        """Reset browser context for fresh retry"""
        try:
            if self.context:
                await self.context.close()
            if self.browser:
                self.context = await self.browser.new_context(
                    viewport={'width': 1920, 'height': 1080},
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                )
                self.page = await self.context.new_page()
                
                if self.stealth_available:
                    from playwright_stealth import stealth_async
                    await stealth_async(self.page)
                
        except Exception as e:
            logger.warning(f"Browser context reset failed: {e}")
    
    def _extract_domain(self, url: str) -> str:
        """Extract domain from URL for strategy lookup"""
        try:
            from urllib.parse import urlparse
            domain = urlparse(url).netloc
            return domain.replace('www.', '')
        except:
            return 'unknown'
    
    async def _cleanup(self):
        """Clean up browser resources"""
        try:
            if self.context:
                await self.context.close()
            if self.browser:
                await self.browser.close()
        except Exception as e:
            logger.warning(f"Cleanup failed: {e}")
    
    async def __aenter__(self):
        """Async context manager entry"""
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        await self._cleanup()

    async def save_screenshots_as_fallback(self, url: str, retailer: str, product_code: str) -> List[str]:
        """
        NEW FEATURE: Save Playwright screenshots as fallback images
        This provides a backup when image URL extraction/processing fails
        """
        logger.info(f"üñºÔ∏è Saving Playwright screenshots as image fallback for {retailer}")
        
        try:
            # Setup browser if not already done
            if not self.browser:
                await self._setup_stealth_browser()
            
            # Navigate to the product page
            domain = self._extract_domain(url)
            strategy = self.screenshot_strategies.get(domain, {})
            
            await self.page.goto(url, wait_until='load', timeout=30000)
            await self._wait_for_content(strategy)
            
            # Take high-quality product screenshots
            screenshot_paths = []
            downloads_dir = "downloads"
            os.makedirs(downloads_dir, exist_ok=True)
            
            # Full page screenshot
            full_page_path = f"{downloads_dir}/{retailer}_{product_code}_full_page.png"
            await self.page.screenshot(path=full_page_path, full_page=True, type='png')
            screenshot_paths.append(full_page_path)
            logger.info(f"üì∏ Saved full page screenshot: {full_page_path}")
            
            # Main product element screenshot (if available)
            product_selectors = [
                '.product-image', '.hero-image', '.main-image',
                '[data-testid="product-image"]', '.product-gallery img'
            ]
            
            for selector in product_selectors:
                try:
                    element = await self.page.query_selector(selector)
                    if element:
                        element_path = f"{downloads_dir}/{retailer}_{product_code}_product_element.png"
                        await element.screenshot(path=element_path, type='png')
                        screenshot_paths.append(element_path)
                        logger.info(f"üì∏ Saved product element screenshot: {element_path}")
                        break
                except:
                    continue
            
            # Viewport screenshots at different scroll positions
            scroll_positions = [0, 0.3, 0.7]
            for i, position in enumerate(scroll_positions):
                await self._scroll_to_position(position)
                await asyncio.sleep(1)
                
                viewport_path = f"{downloads_dir}/{retailer}_{product_code}_viewport_{i}.png"
                await self.page.screenshot(path=viewport_path, type='png')
                screenshot_paths.append(viewport_path)
                logger.info(f"üì∏ Saved viewport screenshot: {viewport_path}")
            
            logger.info(f"‚úÖ Saved {len(screenshot_paths)} fallback screenshots for {retailer}")
            return screenshot_paths
            
        except Exception as e:
            logger.error(f"‚ùå Failed to save screenshot fallback: {e}")
            return []
    
    async def get_screenshot_as_base64(self, element_selector: str = None) -> str:
        """
        Get a screenshot as base64 for immediate use
        Useful for real-time image processing fallback
        """
        try:
            if element_selector:
                element = await self.page.query_selector(element_selector)
                if element:
                    screenshot_bytes = await element.screenshot(type='png')
                else:
                    screenshot_bytes = await self.page.screenshot(type='png')
            else:
                screenshot_bytes = await self.page.screenshot(type='png')
            
            import base64
            return base64.b64encode(screenshot_bytes).decode('utf-8')
            
        except Exception as e:
            logger.error(f"Failed to capture screenshot as base64: {e}")
            return ""


# Integration with existing system
class PlaywrightAgentWrapper:
    """
    Wrapper to integrate Playwright agent with existing agent_extractor.py
    Maintains same interface as Browser Use
    """
    
    def __init__(self, config: Dict):
        self.config = config
        self.agent = PlaywrightMultiScreenshotAgent(config)
    
    async def extract_product_data(self, url: str, retailer: str) -> ExtractionResult:
        """Main interface method - replaces Browser Use extraction"""
        async with self.agent:
            return await self.agent.extract_product(url, retailer)


# Performance monitoring
class PlaywrightPerformanceMonitor:
    """Monitor performance improvements vs Browser Use"""
    
    def __init__(self):
        self.metrics = {
            'extraction_times': [],
            'success_rates': {},
            'cost_savings': 0,
            'verification_challenges_solved': 0
        }
    
    def log_extraction(self, retailer: str, time_taken: float, success: bool, cost: float):
        """Log extraction metrics"""
        self.metrics['extraction_times'].append(time_taken)
        
        if retailer not in self.metrics['success_rates']:
            self.metrics['success_rates'][retailer] = {'success': 0, 'total': 0}
        
        self.metrics['success_rates'][retailer]['total'] += 1
        if success:
            self.metrics['success_rates'][retailer]['success'] += 1
    
    def get_performance_summary(self) -> Dict:
        """Get performance summary vs Browser Use baseline"""
        avg_time = sum(self.metrics['extraction_times']) / len(self.metrics['extraction_times']) if self.metrics['extraction_times'] else 0
        
        overall_success = 0
        overall_total = 0
        for retailer_stats in self.metrics['success_rates'].values():
            overall_success += retailer_stats['success']
            overall_total += retailer_stats['total']
        
        success_rate = (overall_success / overall_total * 100) if overall_total > 0 else 0
        
        return {
            'average_extraction_time': avg_time,
            'success_rate_percent': success_rate,
            'improvement_vs_browser_use': {
                'speed': f"{max(0, 60 - avg_time):.1f}s faster" if avg_time < 60 else f"{avg_time - 60:.1f}s slower",
                'success_rate': f"{max(0, success_rate - 40):.1f}% better" if success_rate > 40 else f"{40 - success_rate:.1f}% worse",
                'cost_savings': "~50% less expensive (no multiple LLM calls)"
            }
        } 